# -*- coding: utf-8 -*-
"""DataPrep.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bqB4GJZXp1ccJ9X-zBBsQFajy59xUkhl
"""

import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno
import warnings
warnings.filterwarnings('ignore')
crime = pd.read_csv('C:/Data Analytics/Sem 3/Dataset/Final/SelectedCrimes.csv')
crime.shape
final = crime[crime['Year']>2014]
final.shape #166097
final.isna().sum()
final.Reported_Date = pd.to_datetime(final.Reported_Date )
final.Primary_Type.value_counts()
#Group crimes 
def crimetype(Crime):
    if ('ASSAULT' in Crime):
        return 'ASSAULT'
    else:
        if ('NARCOTICS' in Crime):
            return 'NARCOTICS'
        else:
            if ('HOMICIDE' in Crime):
                return 'HOMICIDE'
            else:
                return 'VIOLATIONS'

final['Crime_Type'] = final['Primary_Type'].apply(crimetype)

final.Crime_Type.value_counts()

import pandas as pd
from pandas.tseries.holiday import USFederalHolidayCalendar as calendar

cal = calendar()
holidays = cal.holidays(start=final['Reported_Date'].min(), end=final['Reported_Date'].max())

final['Holiday'] = final['Reported_Date'].isin(holidays)
#final.head()

final['DayofWeek'] = final['Reported_Date'].dt.weekday
final['WEEKDAY'] = np.where((final['DayofWeek']) < 5,0,1)
#final.head()

def get_part_of_day(hour):
    return (
        "morning" if 5 <= hour <= 11
        else
        "afternoon" if 12 <= hour <= 17
        else
        "evening" if 18 <= hour <= 22
        else
        "night"
    )

final['Time'] = final['HourOfDay'].apply(get_part_of_day)

final = final.drop(columns='Primary_Type')
final = final.rename(columns={'Crime_Type':'Primary_Type'})



final.Primary_Type.value_counts()

#Yearwise types of crime
yeardata = final.groupby(['Primary_Type','Year']).size().reset_index()

yeardata = yeardata.rename(columns={0:'Crime Count'})
yeardata.to_csv('C:\Data Analytics\Sem 3\YearDatatoPlotFinal.csv',index=False)

timedata = final.groupby(['Year','Time','Holiday','WEEKDAY']).size().reset_index()

#Statistics of arrest and non arrest ()
arrest_count = final['Arrest'].value_counts().sort_index()
totalcrimes = final['ID'].count()

print(arrest_count)
arrestdf = arrest_count.rename_axis('unique_values').reset_index(name='counts')
print (arrestdf)


nonarrestper = round((arrestdf.iloc[0,1]/totalcrimes.astype(int))*100,2)
arrestper = round((arrestdf.iloc[1,1].astype(int)/totalcrimes)*100,2)

print(arrestper)
print(nonarrestper)


fig, ax = plt.subplots()
color_palette_list = ['#009ACD','#ADD8E6','#63D1F4', '#0EBFE9',   
                      '#C1F0F6', '#0099CC']
plt.rcParams['font.sans-serif'] = 'Arial'
plt.rcParams['font.family'] = 'sans-serif'
plt.rcParams['text.color'] = '#565656'
plt.rcParams['axes.labelcolor']= '#565656'
plt.rcParams['xtick.color'] = '#565656'
plt.rcParams['ytick.color'] = '#565656'
plt.rcParams['font.size']=12

labels = ['Arrest Percent', 
         'Non-Arrest Percent']

percentages = [arrestper, nonarrestper]
explode=(0.1,0)
ax.pie(percentages, explode=explode, labels=labels,  
       colors=color_palette_list[0:2], autopct='%1.0f%%', 
       shadow=False, startangle=0,   
       pctdistance=1.2,labeldistance=1.4)
ax.axis('equal')
ax.set_title("Chicago Reported Crimes: Arrest vs. Non-Arrest (2018)")
ax.legend(frameon=False, bbox_to_anchor=(1.5,0.8))

#Convert latitude & longitude to geohash
import pygeohash as pgh
final['geohash'] = final.apply(lambda x: pgh.encode(x.Latitude, x.Longitude, precision=5), axis=1)
dist = final.copy()
final.shape
#Extract distances 
distance = dist[['geohash','NearestPoliceDist','NearestSpeedCamDist','NearestRedCamDist']]
distance = distance.groupby(['geohash']).mean().reset_index()
distance.head()
distance = distance.round(2)
#Select required columns from crime dataset
hs = final[['geohash','Primary_Type','Year','Month','Day','WEEKDAY','Holiday','Time']]
hs.shape
#compute the crime rate for a geohash a latitude and longitude belongs to on a monthly basis
school = hs.groupby(['geohash','Primary_Type','Year','Month','WEEKDAY','Holiday','Time']).size().reset_index()
school.head()
schoolrate_df = school.rename(columns={0:'crimescount'})
schoolrate_df.head()
schoolrate_df = schoolrate_df.round(2) #Rounding to nearest place
#Merge distance data with crime
schoolrate_df = schoolrate_df.merge(distance,on=['geohash'],how='left')
schoolrate_df.shape #34440,11
schoolrate_df.columns
schoolrate_df.isna().sum()
schoolrate_df

#Merge speed cams and red cams in that geohash area
scamslocs = pd.read_csv('C:/Data Analytics/Sem 3/Dataset/LocationData/Speed_Camera_Locations_withZip.csv')
rcamlocs = pd.read_csv('C:/Data Analytics/Sem 3/Dataset/LocationData/Red_Camera_Locations_withZip.csv')
scamslocs.head()
rcamlocs.head()
#Convert location co-ordinates to geohash
scamslocs['geohash'] = scamslocs.apply(lambda x: pgh.encode(x.LATITUDE, x.LONGITUDE, precision=5), axis=1)
rcamlocs['geohash'] = rcamlocs.apply(lambda x: pgh.encode(x.LATITUDE, x.LONGITUDE, precision=5), axis=1)
#Find the count red light cameras and speed cameras in a geohash
slocs = scamslocs.groupby(['geohash']).size().reset_index()
rlocs = rcamlocs.groupby(['geohash']).size().reset_index()
#Rename columns
slocs = slocs.rename(columns={0:'SpeedCamCount'})
rlocs = rlocs.rename(columns={0:'RedCamCount'})
#Merge with crime data
hs = schoolrate_df.merge(slocs,on=['geohash'],how='left')
hs = hs.merge(rlocs,on=['geohash'],how='left')
hs.fillna(0,inplace = True) #Fill Speedcams and RedCams with 0 in case of no cam locations in that area
hs.shape 
hs.Year.value_counts()
hs.head()

#We will plot our dependent variable, crime rate for its distribution and outliers
hs['crimescount'].plot(kind='hist')
plt.show() #Right skewed which means positively skewed

hs.boxplot(column=['crimescount'],return_type='axes')
#Thus, we have to transform our variable using log which will make the distribution better 
#and will have not impact our prediction accuracy

#Group by latitude & longitude
hs.head(1)

hs['Year'] = hs['Year'].astype(object)
hs['Month']=hs['Month'].astype(object)
#hs['Day']=hs['Day'].astype(object)
hs['WEEKDAY'] = hs['WEEKDAY'].astype(object)
hs['Holiday'] = hs['Holiday'].astype(object)
hs['Time'] = hs['Time'].astype(object)

hs.corr()

hs.crimescount.describe()

hs.shape
#Merge education data
edu = pd.read_csv('C:/Data Analytics/Sem 3/Dataset/LocationData/Distances/EducationMerged.csv')
edu.head()
list(edu.columns)
#Calculate average columns from Year1 and Year 2 data for each academic year
edu['Avg_Dropout_Rate'] = (edu['One_Year_Dropout_Rate_Year_1_Pct'] + edu['One_Year_Dropout_Rate_Year_2_Pct'])/2
edu['Avg_FreshmanTrack_Rate'] = (edu['Freshmen_On_Track_School_Pct_Year_2']+edu['Freshmen_On_Track_School_Pct_Year_1'])/2
edu['Avg_CollegeEnrollment_Rate'] = (edu['College_Enrollment_School_Pct_Year_2'] + edu['College_Enrollment_School_Pct_Year_1'])/2
edu['Avg_College_Persistence_Rate'] = (edu['College_Persistence_School_Pct_Year_2'] + edu['College_Persistence_School_Pct_Year_1'])/2
#Select only required columns
education=edu[['geohash', 'Year','Avg_Misconduct_Rate',
 'Avg_Suspension_Rate',
 'Avg_Student_Attendance_Rate',
 'Avg_Teacher_Attendance_Rate',
 'Avg_Suspension_Days',
 'SchoolCount',
 'Avg_Dropout_Rate',
 'Avg_FreshmanTrack_Rate',
 'Avg_CollegeEnrollment_Rate',
 'Avg_College_Persistence_Rate','Mobility_Rate_Pct']]
#Merge with crime data
data = hs.merge(education,on=['geohash','Year'],how='left')
data.shape



data.corr()

#Merge Weather Data
weather = pd.read_csv('C:\Data Analytics\Sem 3\Dataset\LocationData\Distances\weatherapi.csv')
weather.head()
#Group by year and month
weatherm = weather.groupby(['Year','Month']).mean().reset_index()
#Merge with the crime data
alldata = data.merge(weatherm,on=['Year','Month'],how='left')
#Check for NA values
alldata.isna().sum()
alldata.Year.value_counts()
alldata=alldata.dropna()
alldata.Year.value_counts()
alldata.shape #33565,28 After merging all data



alldata.to_csv('C:/Data Analytics/Sem 3/Dataset/LocationData/highschoolcountMergedNov30.csv',index=False)