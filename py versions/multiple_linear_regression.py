# -*- coding: utf-8 -*-
"""Multiple Linear Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AMmUge-o_wk_XIVd6666BM9vhZ8qNUhq
"""
# Multiple Linear Regression
# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import  MinMaxScaler
from sklearn import model_selection
from sklearn.model_selection import KFold,cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings("ignore")
from google.colab import files
uploaded = files.upload()
#Read the file
narcotics = pd.read_csv('CrimeMerged.csv')
#Based on feature selection carried out, select only those features
best = narcotics[['geohash', 'Primary_Type', 'Year', 'Month', 'WEEKDAY', 'Holiday',
                     'Time', 'crimescount','Avg_Student_Attendance_Rate', 'Avg_Teacher_Attendance_Rate', 'Mobility_Rate_Pct', 'NearestPoliceDist', 'NearestRedCamDist','RedCamCount', 'SchoolCount', 'avgTemp', 'prcp','wind']]
#Convert the categorical columns to objects before performing one hot encoding
best.Month = best.Month.astype(object)
best.geohash = best.geohash.astype(object)
best.Year = best.Year.astype(object)
best.WEEKDAY = best.WEEKDAY.astype(object)
best.Holiday = best.Holiday.astype(object)
best.Time = best.Time.astype(object)
best.head()
#Scaling only on numerical columns
numericols = ['float64','int64']
numericbest = best.select_dtypes(include=numericols)
from sklearn.preprocessing import StandardScaler,OneHotEncoder
sc = MinMaxScaler()
NUM = sc.fit_transform(numericbest)
#One hot encoding for only categorical columns
ohe    = OneHotEncoder(sparse=False)
cat =  best.select_dtypes('object')
columns_to_encode = cat.columns
encoded_columns =    ohe.fit_transform(cat[columns_to_encode])
#Merge both categorical and numerical features
final_best = np.concatenate([NUM, encoded_columns], axis=1)
#Convert to dataframe as the above transformation led to arrays
final = pd.DataFrame(final_best)
final.shape
bestdf_dummies = pd.get_dummies(best)
final.columns = bestdf_dummies.columns
X = final.drop(columns='crimescount')
y = final.crimescount
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 20)
# Fitting Multiple Linear Regression to the Training set
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
regressor = LinearRegression()
regressor.fit(X_train, y_train)
# Predicting the Test set results
y_pred = regressor.predict(X_test)
from sklearn.metrics import r2_score
#score=r2_score(y_test,y_pred)
print("R2 ",r2_score(y_test,y_pred))
print("MAE ", mean_absolute_error(y_test,y_pred))
print("MSE ",mean_squared_error(y_test,y_pred))
print("RMSE is",np.sqrt(mean_squared_error(y_test,y_pred)))
y_pred
residual = y_test-y_pred
residual
fig, ax = plt.subplots(figsize=(10,5))
_ = ax.scatter(residual, y_pred)
# Set common labels
ax.tick_params(axis="x", labelsize=15)
ax.tick_params(axis="y", labelsize=15)
ax.set_xlabel('Residuals', fontsize=15)
ax.set_ylabel('Predictions', fontsize=15)
#The residual vs predictions is clumped and the behaviour is not random, thus homoscedasticity assumption not satisfied
import scipy as sp
fig, ax = plt.subplots(figsize=(10,5))
_, (__, ___, r) = sp.stats.probplot(residual, plot=ax, fit=True)
ax.tick_params(axis="x", labelsize=15)
ax.tick_params(axis="y", labelsize=15)
ax.set_title('Normal Probability Plot for Errors', fontsize=16)
ax.set_xlabel('Observed Cumulative Probability', fontsize=16)
ax.set_ylabel('Expected Cumulative Probability', fontsize=16)
#The residuals are non-linear and hence linear regression not a suitable model
np.random.seed(80)
X = final.drop(columns='crimescount').values
Y = final.crimescount.values
kfold = model_selection.KFold(n_splits=10, random_state=200, shuffle=True)
lrkf = LinearRegression()
results_kfoldlr = model_selection.cross_val_score(lrkf, X, Y, cv=kfold)
for train_index, test_index in kfold.split(X):
    print("TRAIN:", train_index, "TEST:", test_index)
    X_trainkf, X_testkf = X[train_index], X[test_index]
    y_trainkf, y_testkf = Y[train_index], Y[test_index]
results_kfoldrlr= model_selection.cross_val_score(lrkf, X, Y, cv=kfold)
print("Accuracy: %.2f%%" % (results_kfoldlr.mean()*100.0)) 
clfrfthree = lrkf.fit(X_trainkf, y_trainkf)
kfoldlr = clfrfthree.predict(X_testkf)
print("RMSE is",np.sqrt(mean_squared_error(y_testkf,kfoldlr))) 
print("R2 ",r2_score(y_testkf, kfoldlr))
print("MAE ", mean_absolute_error(y_testkf, kfoldlr))
print("MSE ",mean_squared_error(y_testkf,kfoldlr))